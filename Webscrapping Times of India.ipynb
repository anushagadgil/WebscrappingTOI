{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d124118e-bae1-4c0c-8d39-268651c61e0f",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06028a6d-3439-4158-b905-1e9294d91977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4b1c0-9c3f-4842-91cc-cc26b4591740",
   "metadata": {},
   "source": [
    "Setting up the base URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6262d86-3fcc-4c43-b364-6bbd063a67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://timesofindia.indiatimes.com/2020/1/1/archivelist/year-2020,month-1,starttime-43831.cms\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d5477-4e15-4040-8521-f6e0acdff64f",
   "metadata": {},
   "source": [
    "Fetching the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b3bc4-c334-4fe8-97d4-60def54061c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(base_url)\n",
    "html_content = response.content\n",
    "\n",
    "#BeautifulSoup to parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e80aaba-e3a2-4901-be79-548e105eb561",
   "metadata": {},
   "source": [
    "Extracting the following necessary info :\n",
    "Title\r\n",
    "Date\r\n",
    "Full text\r\n",
    "Summary (if available)\r\n",
    "Tags (if available)\r\n",
    "Keywords (if available)\r\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32db4e4-c81c-4438-a40b-98e2dc6ab728",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_tags = soup.find_all('span')\n",
    "date_pattern = re.compile(r\"(\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{4},\\s+\\d{2}:\\d{2}\\s+\\w{3}\\b)\")\n",
    "articles_data = []\n",
    "if response.status_code == 200:\n",
    "\n",
    "#parsing the html content\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#finding the links\n",
    "    article_links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "#Iterating through article links and extracting information\n",
    "    for link in article_links:\n",
    "        article_url = link['href']\n",
    "\n",
    "#check to ensure its an article link\n",
    "        if article_url.startswith(\"http://timesofindia.indiatimes.com/\"):\n",
    " #print(\"Fetching content for:\", article_url)\n",
    "\n",
    "# Fetching the article content\n",
    "            article = Article(article_url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "\n",
    "# Extracting necessary info information\n",
    "            title = article.title\n",
    "            date = article.publish_date\n",
    "            full_text = article.text\n",
    "            summary = article.summary if article.summary else \"Summary not available\"\n",
    "            keywords = article.keywords\n",
    "            tags = article.tags if article.tags else []\n",
    "# Displaying the extracted information\n",
    "            print(\"Title:\", title)\n",
    "            for tag in span_tags:\n",
    "                if tag.text.strip():\n",
    "                   match = date_pattern.search(tag.text.strip())\n",
    "                   if match:\n",
    "                       print(\"Date:\", match.group())\n",
    "            print(\"Summary:\", summary)\n",
    "            print(\"Tags:\", tags)\n",
    "            print(\"Keywords:\", keywords)\n",
    "            print(\"URL:\", article_url)\n",
    "            print(\"--------------------------------------------------\")\n",
    "\n",
    "# Break after printing the first article\n",
    "            break\n",
    "\n",
    "else:\n",
    "    print(\"Failed to fetch the archive page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8546f1-9335-440f-8837-dea049b28621",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_data = []\n",
    "\n",
    "if response.status_code == 200:\n",
    "#links to archive pages\n",
    "    article_links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "#Iterating through article links and extracting info\n",
    "    for link in article_links:\n",
    "        article_url = link['href']\n",
    "\n",
    "# Checking if it is an article link\n",
    "        if article_url.startswith(\"http://timesofindia.indiatimes.com/\"):\n",
    "            print(\"Fetching content for:\", article_url)\n",
    "\n",
    "# Fetching the article content\n",
    "            article = Article(article_url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "\n",
    "# Extracting additional information\n",
    "            title = article.title\n",
    "            full_text = article.text\n",
    "            summary = article.summary if article.summary else \"Summary not available\"\n",
    "            keywords = list(article.keywords)  # Convert set to list\n",
    "            tags = list(article.tags)  # Convert set to list\n",
    "\n",
    "# Extracting the date from span tags\n",
    "            date = None\n",
    "            for tag in span_tags:\n",
    "                if tag.text.strip():\n",
    "                    match = date_pattern.search(tag.text.strip())\n",
    "                    if match:\n",
    "                        date = match.group()\n",
    "                        break\n",
    "\n",
    "# Store article information in a dictionary\n",
    "            article_info = {\n",
    "                \"Title\": title,\n",
    "                \"Date\": date,\n",
    "                \"Summary\": summary,\n",
    "                \"Tags\": tags,\n",
    "                \"Keywords\": keywords,\n",
    "                \"URL\": article_url\n",
    "            }\n",
    "\n",
    "# Appending article information to the list\n",
    "            articles_data.append(article_info)\n",
    "\n",
    "# Break after fetching the first article\n",
    "            break\n",
    "\n",
    "else:\n",
    "    print(\"Failed to fetch the archive page.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd528a0-2030-4049-9fa7-d921b9bb9082",
   "metadata": {},
   "source": [
    "Saving the data in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3c97b-9f02-4142-a0ba-ecae626ddab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the JSON file\n",
    "json_file_path = \"C:\\\\Users\\\\Anusha Gadgil\\\\Desktop\\\\RA stuff\\\\January_2020.json\"\n",
    "\n",
    "# Write the article data to a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(articles_data, json_file, indent=4)\n",
    "\n",
    "print(\"Data saved to:\", json_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
